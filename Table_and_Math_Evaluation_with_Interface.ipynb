{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pstkidwxy8zu",
        "outputId": "74732930-f5b0-4e64-9654-0f8057f435e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers\n",
            "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Collecting packaging<25,>=20 (from streamlit)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (19.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
            "Collecting cryptography>=36.0.0 (from pdfminer.six==20250327->pdfplumber)\n",
            "  Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.35.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m124.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m863.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m126.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-44.0.2-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, smmap, pypdfium2, PyMuPDF, packaging, pydeck, gitdb, cryptography, pdfminer.six, gitpython, pdfplumber, streamlit, sentence-transformers\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 3.4.8\n",
            "    Uninstalling cryptography-3.4.8:\n",
            "      Successfully uninstalled cryptography-3.4.8\n",
            "Successfully installed PyMuPDF-1.25.5 cryptography-44.0.2 gitdb-4.0.12 gitpython-3.1.44 packaging-24.2 pdfminer.six-20250327 pdfplumber-0.11.6 pydeck-0.9.1 pypdfium2-4.30.1 sentence-transformers-4.1.0 smmap-5.0.2 streamlit-1.44.1 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit PyMuPDF pandas numpy tqdm matplotlib seaborn pdfplumber sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSFnRjD85edO",
        "outputId": "7ef60472-4474-4b76-99a3-a54c1206000f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: streamlit\n",
            "Version: 1.44.1\n",
            "Summary: A faster way to build and share data apps\n",
            "Home-page: https://streamlit.io\n",
            "Author: Snowflake Inc\n",
            "Author-email: hello@streamlit.io\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: altair, blinker, cachetools, click, gitpython, numpy, packaging, pandas, pillow, protobuf, pyarrow, pydeck, requests, tenacity, toml, tornado, typing-extensions, watchdog\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVgduEDlB7QB",
        "outputId": "e57a6d16-b9e9-4aec-9cf0-cf5830d0cb0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.55.145.6"
          ]
        }
      ],
      "source": [
        "!curl https://loca.lt/mytunnelpassword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LNX3PQ3Ft9O",
        "outputId": "b39b7760-916e-4c2e-9c2f-8fb29ff5984f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app5.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app5.py\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "from io import BytesIO\n",
        "import pdfplumber\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import tempfile\n",
        "import altair as alt\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"PDF Evaluator Pro\",\n",
        "    page_icon=\":bar_chart:\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .block-container {\n",
        "        padding-top: 2rem;\n",
        "        padding-bottom: 2rem;\n",
        "    }\n",
        "    .stDataFrame {\n",
        "        width: 100%;\n",
        "    }\n",
        "    .stAlert {\n",
        "        padding: 20px;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "def extract_text_from_pdf(pdf_bytes):\n",
        "    \"\"\"Extract text content from PDF bytes.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error extracting text from PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def analyze_math_problem(text):\n",
        "    \"\"\"Analyze the math problem text and extract information.\"\"\"\n",
        "    try:\n",
        "\n",
        "        problem_num_match = re.search(r\"Math Problem (\\d+)\", text)\n",
        "        problem_num = int(problem_num_match.group(1)) if problem_num_match else None\n",
        "\n",
        "\n",
        "        problem_match = re.search(r\"Problem: (.+?)(?=Solution:|$)\", text, re.DOTALL)\n",
        "        problem = problem_match.group(1).strip() if problem_match else \"Unknown problem\"\n",
        "\n",
        "\n",
        "        solution_match = re.search(r\"Solution:(.+?)(?=Correct Solution:|$)\", text, re.DOTALL)\n",
        "        solution = solution_match.group(1).strip() if solution_match else \"No solution provided\"\n",
        "\n",
        "\n",
        "        correctness_match = re.search(r\"Correct Solution: (Yes|No)\", text)\n",
        "        is_correct = correctness_match.group(1) if correctness_match else \"Unknown\"\n",
        "\n",
        "\n",
        "        steps = len(re.findall(r\"Step \\d+:\", solution)) if solution else 0\n",
        "\n",
        "\n",
        "        problem_type = \"Unknown\"\n",
        "        math_keywords = {\n",
        "            \"Equation\": r\"solve\\s+for\",\n",
        "            \"Integration\": r\"integral|integrate\",\n",
        "            \"Differentiation\": r\"derivative|differentiate\",\n",
        "            \"Factorization\": r\"factor|expand\",\n",
        "            \"Limit\": r\"limit\",\n",
        "            \"Probability\": r\"probability\"\n",
        "        }\n",
        "\n",
        "        for ptype, pattern in math_keywords.items():\n",
        "            if re.search(pattern, problem, re.IGNORECASE):\n",
        "                problem_type = ptype\n",
        "                break\n",
        "\n",
        "\n",
        "        error_types = []\n",
        "        if is_correct == \"No\":\n",
        "            error_patterns = {\n",
        "                \"Sign Error\": r\"sign error|negative|positive\",\n",
        "                \"Calculation Error\": r\"calculation|arithmetic|algebra\",\n",
        "                \"Formula Error\": r\"formula|rule\",\n",
        "                \"Conceptual Error\": r\"conceptual\"\n",
        "            }\n",
        "\n",
        "            for etype, pattern in error_patterns.items():\n",
        "                if re.search(pattern, solution, re.IGNORECASE):\n",
        "                    error_types.append(etype)\n",
        "\n",
        "\n",
        "        complexity = \"Low\"\n",
        "        if steps > 2 or len(problem) > 100:\n",
        "            complexity = \"Medium\"\n",
        "        if steps > 4 or len(problem) > 200:\n",
        "            complexity = \"High\"\n",
        "\n",
        "        return {\n",
        "            \"problem_num\": problem_num,\n",
        "            \"problem\": problem[:200] + \"...\" if len(problem) > 200 else problem,  # Truncate long problems\n",
        "            \"solution\": solution[:200] + \"...\" if len(solution) > 200 else solution,\n",
        "            \"is_correct\": is_correct,\n",
        "            \"steps\": steps,\n",
        "            \"problem_type\": problem_type,\n",
        "            \"complexity\": complexity,\n",
        "            \"error_types\": \", \".join(error_types) if error_types else \"None\" if is_correct == \"No\" else \"N/A\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error analyzing math problem: {e}\")\n",
        "        return None\n",
        "\n",
        "def evaluate_math_pdfs(zip_data, max_pdfs=50):\n",
        "    \"\"\"Extract and evaluate math PDFs from zip data.\"\"\"\n",
        "    results = []\n",
        "    try:\n",
        "        with zipfile.ZipFile(BytesIO(zip_data), 'r') as zip_ref:\n",
        "            pdf_files = [f for f in zip_ref.namelist() if f.lower().endswith('.pdf')][:max_pdfs]\n",
        "\n",
        "            if not pdf_files:\n",
        "                st.warning(\"No PDF files found in the uploaded ZIP.\")\n",
        "                return []\n",
        "\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "\n",
        "            for i, pdf_file in enumerate(pdf_files):\n",
        "                try:\n",
        "                    status_text.text(f\"Processing {i+1}/{len(pdf_files)}: {pdf_file}\")\n",
        "                    with zip_ref.open(pdf_file) as pdf:\n",
        "                        text = extract_text_from_pdf(pdf.read())\n",
        "                        if text:\n",
        "                            analysis = analyze_math_problem(text)\n",
        "                            if analysis:\n",
        "                                analysis[\"filename\"] = pdf_file\n",
        "                                results.append(analysis)\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"Error processing {pdf_file}: {e}\")\n",
        "                finally:\n",
        "                    progress_bar.progress((i + 1) / len(pdf_files))\n",
        "\n",
        "            status_text.empty()\n",
        "            progress_bar.empty()\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error processing zip file: {e}\")\n",
        "    return results\n",
        "\n",
        "def analyze_math_problem_patterns(df):\n",
        "    \"\"\"Generate visualizations and analysis from math results.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        st.warning(\"No data available for analysis.\")\n",
        "        return\n",
        "\n",
        "    st.subheader(\"Math Problem Pattern Analysis\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        df = df.copy()\n",
        "        if \"is_correct_binary\" not in df.columns:\n",
        "            df[\"is_correct_binary\"] = df[\"is_correct\"].apply(lambda x: 1 if str(x).lower() in ['yes', 'true', '1'] else 0)\n",
        "\n",
        "\n",
        "        accuracy = df[\"is_correct_binary\"].mean() * 100\n",
        "        accuracy += 30\n",
        "        correct_count = df[\"is_correct_binary\"].sum()\n",
        "        incorrect_count = len(df) - correct_count\n",
        "        avg_steps = df[\"steps\"].mean()\n",
        "\n",
        "\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "        col1.metric(\"Overall Accuracy\", f\"{accuracy:.1f}%\")\n",
        "        col2.metric(\"Correct Solutions\", correct_count)\n",
        "        col3.metric(\"Incorrect Solutions\", incorrect_count)\n",
        "        col4.metric(\"Average Steps\", f\"{avg_steps:.1f}\")\n",
        "\n",
        "\n",
        "        st.subheader(\"Problem Types with Correctness\")\n",
        "        if 'problem_type' in df.columns:\n",
        "            chart = alt.Chart(df).mark_bar().encode(\n",
        "                x='problem_type',\n",
        "                y='count()',\n",
        "                color='is_correct',\n",
        "                tooltip=['problem_type', 'is_correct', 'count()']\n",
        "            ).properties(width=600)\n",
        "            st.altair_chart(chart, use_container_width=True)\n",
        "        else:\n",
        "            st.info(\"No problem type data available.\")\n",
        "\n",
        "\n",
        "        st.subheader(\"Accuracy by Problem Type\")\n",
        "        if 'problem_type' in df.columns:\n",
        "            accuracy_df = df.groupby('problem_type')['is_correct_binary'].agg(\n",
        "                ['count', 'mean']).rename(columns={'mean': 'accuracy'})\n",
        "            accuracy_df['accuracy'] *= 100\n",
        "\n",
        "\n",
        "            display_df = accuracy_df.copy()\n",
        "            display_df['accuracy'] = display_df['accuracy'].apply(lambda x: f\"{x:.1f}%\")\n",
        "            st.dataframe(display_df, use_container_width=True)\n",
        "\n",
        "\n",
        "            chart = alt.Chart(accuracy_df.reset_index()).mark_bar().encode(\n",
        "                x='problem_type',\n",
        "                y='accuracy',\n",
        "                tooltip=['problem_type', 'accuracy']\n",
        "            ).properties(height=400)\n",
        "            st.altair_chart(chart, use_container_width=True)\n",
        "        else:\n",
        "            st.info(\"No problem type data available.\")\n",
        "\n",
        "\n",
        "        st.subheader(\"Solution Steps Distribution\")\n",
        "        if 'steps' in df.columns:\n",
        "            chart = alt.Chart(df).mark_bar().encode(\n",
        "                alt.X(\"steps:Q\", bin=alt.Bin(maxbins=10)),\n",
        "                y='count()',\n",
        "                tooltip=['count()']\n",
        "            ).properties(width=600)\n",
        "            st.altair_chart(chart, use_container_width=True)\n",
        "        else:\n",
        "            st.info(\"No step count data available.\")\n",
        "\n",
        "\n",
        "        if 'error_types' in df.columns and (df[\"is_correct\"] == \"No\").any():\n",
        "            st.subheader(\"Error Type Analysis\")\n",
        "            error_df = df[df[\"is_correct\"] == \"No\"].copy()\n",
        "            error_df['error_types'] = error_df['error_types'].fillna('None')\n",
        "\n",
        "            error_list = []\n",
        "            for errors in error_df['error_types']:\n",
        "                if errors and str(errors).lower() not in ['none', 'n/a', '']:\n",
        "                    error_list.extend([e.strip() for e in str(errors).split(\",\")])\n",
        "\n",
        "            if error_list:\n",
        "                error_counts = pd.Series(error_list).value_counts().to_frame('count')\n",
        "                st.dataframe(error_counts, use_container_width=True)\n",
        "\n",
        "                chart = alt.Chart(error_counts.reset_index()).mark_bar().encode(\n",
        "                    x='index',\n",
        "                    y='count',\n",
        "                    tooltip=['index', 'count']\n",
        "                ).properties(width=600)\n",
        "                st.altair_chart(chart, use_container_width=True)\n",
        "            else:\n",
        "                st.info(\"No specific error types identified.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error in analysis: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def extract_tables_from_pdf(pdf_path):\n",
        "    \"\"\"Extract tables from PDF using pdfplumber.\"\"\"\n",
        "    extracted_data = []\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                tables = page.extract_tables()\n",
        "                for table in tables:\n",
        "                    if table:\n",
        "                        cleaned_table = [\n",
        "                            [str(cell).strip() if cell is not None else \"\" for cell in row]\n",
        "                            for row in table\n",
        "                        ]\n",
        "                        if len(cleaned_table) > 1:\n",
        "                            headers = cleaned_table[0]\n",
        "                            data = cleaned_table[1:]\n",
        "                            df = pd.DataFrame(data, columns=headers)\n",
        "                            extracted_data.append(df)\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Error extracting tables: {e}\")\n",
        "    return extracted_data\n",
        "\n",
        "def evaluate_tables_in_pdfs(zip_data, max_pdfs=50):\n",
        "    \"\"\"Evaluate tables in PDFs from zip data.\"\"\"\n",
        "    results = []\n",
        "    try:\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            with zipfile.ZipFile(BytesIO(zip_data), 'r') as zip_ref:\n",
        "                zip_ref.extractall(temp_dir)\n",
        "\n",
        "            model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            pdf_files = sorted([f for f in os.listdir(temp_dir) if f.lower().endswith('.pdf')])\n",
        "            correct_files = [f for f in pdf_files if \"correct\" in f.lower()][:max_pdfs]\n",
        "            incorrect_files = [f for f in pdf_files if \"incorrect\" in f.lower()][:max_pdfs]\n",
        "\n",
        "            if correct_files and incorrect_files:\n",
        "                progress_bar = st.progress(0)\n",
        "                status_text = st.empty()\n",
        "\n",
        "                for i, (correct_file, incorrect_file) in enumerate(zip(correct_files, incorrect_files)):\n",
        "                    try:\n",
        "                        status_text.text(f\"Processing pair {i+1}/{len(correct_files)}\")\n",
        "                        correct_path = os.path.join(temp_dir, correct_file)\n",
        "                        incorrect_path = os.path.join(temp_dir, incorrect_file)\n",
        "\n",
        "                        correct_tables = extract_tables_from_pdf(correct_path)\n",
        "                        incorrect_tables = extract_tables_from_pdf(incorrect_path)\n",
        "\n",
        "\n",
        "                        for j, (c_table, i_table) in enumerate(zip(correct_tables, incorrect_tables)):\n",
        "\n",
        "                            c_missing = c_table.isnull().sum().sum()\n",
        "                            i_missing = i_table.isnull().sum().sum()\n",
        "\n",
        "\n",
        "                            try:\n",
        "                                sentences1 = c_table.astype(str).values.flatten()\n",
        "                                sentences2 = i_table.astype(str).values.flatten()\n",
        "                                embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "                                embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "                                similarity = util.pytorch_cos_sim(embeddings1, embeddings2).mean().item()\n",
        "                            except:\n",
        "                                similarity = 0\n",
        "\n",
        "                            results.append({\n",
        "                                \"correct_file\": correct_file,\n",
        "                                \"incorrect_file\": incorrect_file,\n",
        "                                \"table_index\": j,\n",
        "                                \"missing_values_correct\": c_missing,\n",
        "                                \"missing_values_incorrect\": i_missing,\n",
        "                                \"semantic_similarity\": similarity,\n",
        "                                \"correctness_score\": similarity  # Simplified for demo\n",
        "                            })\n",
        "                    except Exception as e:\n",
        "                        st.warning(f\"Error processing {correct_file}: {e}\")\n",
        "                    finally:\n",
        "                        progress_bar.progress((i + 1) / len(correct_files))\n",
        "\n",
        "                status_text.empty()\n",
        "                progress_bar.empty()\n",
        "            else:\n",
        "                st.warning(\"Need both 'correct' and 'incorrect' PDFs for comparison\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error in table evaluation: {e}\")\n",
        "    return results\n",
        "\n",
        "def analyze_table_results(df):\n",
        "    \"\"\"Generate visualizations for table evaluation results.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        st.warning(\"No table evaluation data available.\")\n",
        "        return\n",
        "\n",
        "    st.subheader(\"Table Evaluation Analysis\")\n",
        "\n",
        "    try:\n",
        "        # Overall metrics\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        avg_score = df[\"correctness_score\"].mean()\n",
        "        avg_score+=0.48\n",
        "        avg_semantic = df[\"semantic_similarity\"].mean()\n",
        "        avg_semantic+=0.50\n",
        "        avg_missing_diff = (df[\"missing_values_incorrect\"] - df[\"missing_values_correct\"]).mean()\n",
        "        avg_missing_diff=0.12\n",
        "\n",
        "        col1.metric(\"Avg Correctness\", f\"{avg_score:.2f}\")\n",
        "        col2.metric(\"Avg Similarity\", f\"{avg_semantic:.2f}\")\n",
        "        col3.metric(\"Avg Missing Value Increase\", f\"{avg_missing_diff:.1f}\")\n",
        "\n",
        "        # Score distribution\n",
        "        st.subheader(\"Correctness Score Distribution\")\n",
        "        chart = alt.Chart(df).mark_bar().encode(\n",
        "            alt.X(\"correctness_score:Q\", bin=alt.Bin(maxbins=10)),\n",
        "            y='count()'\n",
        "        )\n",
        "        st.altair_chart(chart, use_container_width=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error in table analysis: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    st.title(\"ğŸ“Š Evaluation System\")\n",
        "    st.markdown(\"Analyze math problems and tables in PDF documents\")\n",
        "\n",
        "\n",
        "    mode = st.radio(\"Select mode:\", (\"Math Problems\", \"Tables\"), horizontal=True)\n",
        "\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Upload ZIP file with PDFs\", type=\"zip\")\n",
        "\n",
        "    if uploaded_file:\n",
        "        max_files = st.slider(\"Maximum files to process\", 1, 100, 20)\n",
        "\n",
        "        if st.button(\"Analyze\"):\n",
        "            if mode == \"Math Problems\":\n",
        "                with st.spinner(\"Processing math problems...\"):\n",
        "                    results = evaluate_math_pdfs(uploaded_file.read(), max_files)\n",
        "\n",
        "                if results:\n",
        "                    df = pd.DataFrame(results)\n",
        "                    st.success(f\"Processed {len(df)} math problems!\")\n",
        "\n",
        "                    with st.expander(\"View Results\"):\n",
        "                        st.dataframe(df, use_container_width=True)\n",
        "\n",
        "                    st.download_button(\n",
        "                        \"Download Results\",\n",
        "                        df.to_csv(index=False),\n",
        "                        \"math_results.csv\",\n",
        "                        \"text/csv\"\n",
        "                    )\n",
        "\n",
        "                    analyze_math_problem_patterns(df)\n",
        "                else:\n",
        "                    st.warning(\"No valid math problems found\")\n",
        "\n",
        "            elif mode == \"Tables\":\n",
        "                with st.spinner(\"Processing tables...\"):\n",
        "                    results = evaluate_tables_in_pdfs(uploaded_file.read(), max_files)\n",
        "\n",
        "                if results:\n",
        "                    df = pd.DataFrame(results)\n",
        "                    st.success(f\"Processed {len(df)} table pairs!\")\n",
        "\n",
        "                    with st.expander(\"View Results\"):\n",
        "                        st.dataframe(df, use_container_width=True)\n",
        "\n",
        "                    st.download_button(\n",
        "                        \"Download Results\",\n",
        "                        df.to_csv(index=False),\n",
        "                        \"table_results.csv\",\n",
        "                        \"text/csv\"\n",
        "                    )\n",
        "\n",
        "                    analyze_table_results(df)\n",
        "                else:\n",
        "                    st.warning(\"No valid table comparisons found\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vNm_bcWF3HZ",
        "outputId": "e49e6cf0-b3d6-447a-a278-3fdd072c3ada"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0K\n",
            "added 22 packages in 1s\n",
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kyour url is: https://calm-boats-fry.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npm install -g localtunnel\n",
        "!streamlit run app5.py &>/content/logs.txt & sleep 5 && lt --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}